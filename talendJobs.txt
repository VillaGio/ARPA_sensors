 
|| csvBulkLoad ||

Performs bulk instert of sensors data from 1968 to 2021 retrieved from csv downloaded from the ARPA website 


--PREJOB
-->START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



1995 tFileInputDelimited: retrieves data from 1968 to 1965 saved in 1995.csv 
-->
CHNAGE_DATA_95 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
1995 tDbOutputBulk: generates a new csv -mod_1995.csv- whit correct structure for bulk insert


2000 tFileInputDelimited: retrieves data from 1996 to 2000 saved in 2000.csv 
-->
CHNAGE_DATA_00 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2000 tDbOutputBulk: generates a new csv -mod_2000.csv- whit correct structure for bulk insert


2004 tFileInputDelimited: retrieves data from 2001 to 2004 saved in 2004.csv 
-->
CHNAGE_DATA_04 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2004 tDbOutputBulk: generates a new csv -mod_2004.csv- whit correct structure for bulk insert


2007 tFileInputDelimited: retrieves data from 2005 to 2007 saved in 2007.csv 
-->
CHNAGE_DATA_07 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2007 tDbOutputBulk: generates a new csv -mod_2007.csv- whit correct structure for bulk insert


2010 tFileInputDelimited: retrieves data from 1968 to 1965 saved in 2010.csv 
-->
CHNAGE_DATA_10 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2010 tDbOutputBulk: generates a new csv -mod_2010.csv- whit correct structure for bulk insert



LOOP_11_21 tLoop: like a "for cycle" -> for i in range(2011, 2021): do what's next
	2011_2021 tFileInputDelimited: retrieves data from i year saved in i.csv 
	-->
	CHNAGE_DATA_11_21 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
	-->
	NO_DUPLICATES tUniqRow: filters out any perfectly duplicated row
	-->
	2011-2021 tDbOutputBulk: generates a new csv -mod_i.csv- whit correct structure for bulk insert
	-->
	wait3secs tSleep: sleep for 3 secs to give the system some time



INSERT1995 tDBBulkExc: performs a bulk insert of data stored in mod_1995.csv in Postgres table "sens_data_1995"
-->
INSERT2000 tDBBulkExc: performs a bulk insert of data stored in mod_2000.csv in Postgres table "sens_data_2000"
-->
....same....
-->
COMMIT tDBCommit: commits the changes made to Postgres and closes connection to Postgres woneli instance	 

___________________________________________________________________________________



|| csv2022Load ||

Performs bulk instert of sensors data about 2022 retrieved from api (no csv bc thar had duplicates)


--PREJOB
-->START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



API_REQ_2022 tRest: retrieve sensors data about 2022 from the ARPA api
-->
CONVERT_DATATYPES tConvertType: converts the types of fields extracted from the api, from all strings to the correct type for the databse
-->
SET_DATE tMap: modifies the data format from 2022-01-01T00:00:00.000 to 2022-01-01 00:00:00
-->
2022_csv tFileOutputDelimited: writes the data into 2022_api.csv

-->
wait5secs tSleep: sleep for 5 secs to give the system the time to get the csv
-->

2022 tfileInputDelimited: retrieves data saved in 2022_api.csv
-->
MANAGE_ORA_LEGALE tMap: changes Data field where value is "2022-03-27 02:00:00" to "2022-03-27 01:59:00" to avoid dupicates related to changing hour
-->
2022 tDbOutputBulk: generates a new csv -mod_2022_api.csv- whit correct structure for bulk insert

-->
wait5secs tSleep: sleep for 5 secs to give the system the time to get the csv
-->

INSERT2022 tDBBulkExc: performs a bulk insert of data stored in mod_2022_api.csv in Postgres table "sens_data_2022"
-->
COMMIT tDBCommit: commits the changes made to Postgres



--POSTJOB
--> CLOSE_CONNECTIONS tDBClose: closes connection to Postgres woneli instance	

___________________________________________________________________________________



|| deltaIngestion2022 ||

Performs delta ingestion of sensors data about 2022 retrieved from api, i.e. check for changes in the api (new, updated, and deleted records) 
to be mirrored in the Postgres table "sens_data_2022" firstly populated with the previous job. 
It also saves information to be written in the "logs" table on Postgres.
This job is scheduled to run automatically everyday at 18.35 in Windows Task Scheduler.


--PREJOB
--> START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



API_REQUEST tHttpRequest: retrieve sensors data about 2022 from the ARPA api. 
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. All fields are exctracted and imported as String type 
-->
CHANGE_ORA tMap: changes Data field where value is "2022-03-27T02:00:00.000" to "2022-03-27T01:59:00.000" to avoid dupicates related to changing hour
-->
CONVERT_DATATYPES tConvertType: convert each field to the respective datatype (Long,Date,Float,String,Integer). 
					  This final flow will have the role of LEFT table in the later join 
-->
SENS_DATA_2022 tDBInput: retrieves data from table "sens_data_2022" on Postgres. 
				 This flow will have the role of RIGHT table in the later join 
-->
JOIN tMap: performs an INNER JOIN with data from api as left and sens_data_2022 as right; join keys are both idsensore and dataora.
	     the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
	     Generates two output tables:
		- INSERT: obtained from the REJECT of the join operation, i.e., records that are in the api that are not in the db table 
				-->
				INSERT_SHOW tLogRow: shows records to be insert
				-->
				INSERT tDBOutput:	performs an insert operation in Postgres table "sens_data_2022"		
				
		- NOT INSERT: obtained from the MAIN of the join operation, i.e., records present both in the api and the db table.
				  These could need to be updated or not, according to whether their fields are equal. To screen that,
				  a new boolean field "update" is created, checking whether to records are equal (TRUE) or not (FALSE)
					-->
					UPDATE_FILTER tMap: generates an output table with filtered records: only those with "update" field = True pass.
					-->
					UPDATE_SHOW_API tLogRow: shows records to be updated
					-->
					UPDATE tDBOutput: performs an update operation in Postgres table "sens_data_2022"	
	
 
----- EXECUTE AFTER PREVIOUS SUBJOB

SENS_DATA_2022 tDBInput: retrieves data from table "sens_data_2022" on Postgres. 
				 This flow will have the role of LEFT table in the later join
-->
API_REQUEST tHttpRequest: retrieve sensors data about 2022 from the ARPA api. 
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. All fields are exctracted and imported as String type 
-->
CHANGE_ORA tMap: changes Data field where value is "2022-03-27T02:00:00.000" to "2022-03-27T01:59:00.000" to avoid dupicates related to changing hour
-->
CONVERT_DATATYPES tConvertType: convert each field to the respective datatype (Long,Date,Float,String,Integer). 
					  This final flow will have the role of RIGHT table in the later join 
-->
JOIN tMap: performs an INNER JOIN with data from sens_data_2022 as left and data from api as right; join keys are both idsensore and dataora.
	     the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
	     Generates one output table:
		- DELETED: obtained from the REJECT of the join operation, i.e., records that are in the db table that are not in the api
				-->
				DELETE_SHOW tLogRow: shows records to be deleted
				-->
				DELETE tDBOutput:	performs a delete operation in Postgres table "sens_data_2022"		



--POSTJOB
--> PLACEHOLDER tJava: empty tJava functioning as placeholder to connect the followed components (that cannot be directly linked to the PREJOB)
	
	-->
	UPDATE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_UPDATE tMap: creates a boolean variable "update_check" checking whether the number of lines passed in the UPDATE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "update_check" field = True pass.
				     This way, only if some update actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	UPDATE_LOG tDBOutput: performs an insert operation in Postgres table "logs"

	

	-->
	INSERT tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_INSERT tMap: creates a boolean variable "insert_check" checking whether the number of lines passed in the INSERT tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "insert_check" field = True pass.
				     This way, only if some insert actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	INSERT_LOG tDBOutput: performs an insert operation in Postgres table "logs"



	-->
	DELETE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_DELETE tMap: creates a boolean variable "delete_check" checking whether the number of lines passed in the DELETE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "delete_check" field = True pass.
				     This way, only if some delete actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	DELETE_LOG tDBOutput: performs an insert operation in Postgres table "logs"


-->
COMMIT tDBCommit: commits the changes made to Postgres and closes connection to Postgres woneli instance

___________________________________________________________________________________



|| sensorsPositionHistory ||



--PREJOB
--> START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



API_REQUEST tHttpRequest:
