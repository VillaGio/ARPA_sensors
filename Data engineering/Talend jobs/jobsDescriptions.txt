 
|| csvBulkLoad ||

Performs bulk instert of sensors data from 1968 to 2021 retrieved from csv downloaded from the ARPA website 


--PREJOB
-->START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



1995 tFileInputDelimited: retrieves data from 1968 to 1965 saved in 1995.csv 
-->
CHNAGE_DATA_95 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
1995 tDbOutputBulk: generates a new csv -mod_1995.csv- whit correct structure for bulk insert


2000 tFileInputDelimited: retrieves data from 1996 to 2000 saved in 2000.csv 
-->
CHNAGE_DATA_00 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2000 tDbOutputBulk: generates a new csv -mod_2000.csv- whit correct structure for bulk insert


2004 tFileInputDelimited: retrieves data from 2001 to 2004 saved in 2004.csv 
-->
CHNAGE_DATA_04 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2004 tDbOutputBulk: generates a new csv -mod_2004.csv- whit correct structure for bulk insert


2007 tFileInputDelimited: retrieves data from 2005 to 2007 saved in 2007.csv 
-->
CHNAGE_DATA_07 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2007 tDbOutputBulk: generates a new csv -mod_2007.csv- whit correct structure for bulk insert


2010 tFileInputDelimited: retrieves data from 1968 to 1965 saved in 2010.csv 
-->
CHNAGE_DATA_10 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
-->
NO_DUPLICATES tUniqRow: filters out any perfect duplicated row
-->
2010 tDbOutputBulk: generates a new csv -mod_2010.csv- whit correct structure for bulk insert



LOOP_11_21 tLoop: like a "for cycle" -> for i in range(2011, 2021): do what's next
	2011_2021 tFileInputDelimited: retrieves data from i year saved in i.csv 
	-->
	CHNAGE_DATA_11_21 tMap: changes Data column where fields correspond to changing hour night from "yyyy-MM-dd 02:00:00" to "yyyy-MM-dd 01:59:00" to avoid dupicates related to changing hour
	-->
	NO_DUPLICATES tUniqRow: filters out any perfectly duplicated row
	-->
	2011-2021 tDbOutputBulk: generates a new csv -mod_i.csv- whit correct structure for bulk insert
	-->
	wait3secs tSleep: sleep for 3 secs to give the system some time



INSERT1995 tDBBulkExc: performs a bulk insert of data stored in mod_1995.csv in Postgres table "sens_data_1995"
-->
INSERT2000 tDBBulkExc: performs a bulk insert of data stored in mod_2000.csv in Postgres table "sens_data_2000"
-->
....same....
-->
COMMIT tDBCommit: commits the changes made to Postgres and closes connection to Postgres woneli instance	 

___________________________________________________________________________________



|| csv2022Load ||

Performs bulk instert of sensors data about 2022 retrieved from api (no csv bc thar had duplicates)


--PREJOB
-->START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



API_REQ_2022 tRest: retrieve sensors data about 2022 from the ARPA api
-->
CONVERT_DATATYPES tConvertType: converts the types of fields extracted from the api, from all strings to the correct type for the databse
-->
SET_DATE tMap: modifies the data format from 2022-01-01T00:00:00.000 to 2022-01-01 00:00:00
-->
2022_csv tFileOutputDelimited: writes the data into 2022_api.csv

-->
wait5secs tSleep: sleep for 5 secs to give the system the time to get the csv
-->

2022 tfileInputDelimited: retrieves data saved in 2022_api.csv
-->
MANAGE_ORA_LEGALE tMap: changes Data field where value is "2022-03-27 02:00:00" to "2022-03-27 01:59:00" to avoid dupicates related to changing hour
-->
2022 tDbOutputBulk: generates a new csv -mod_2022_api.csv- whit correct structure for bulk insert

-->
wait5secs tSleep: sleep for 5 secs to give the system the time to get the csv
-->

INSERT2022 tDBBulkExc: performs a bulk insert of data stored in mod_2022_api.csv in Postgres table "sens_data_2022"
-->
COMMIT tDBCommit: commits the changes made to Postgres



--POSTJOB
--> CLOSE_CONNECTIONS tDBClose: closes connection to Postgres woneli instance	

___________________________________________________________________________________



|| deltaIngestion2022 ||

Performs delta ingestion of sensors data about 2022 retrieved from api, i.e. checks for changes in the api (new, updated, and deleted records) 
to be mirrored in the Postgres table "sens_data_2022" firstly populated with the previous job. 
It also saves information to be written in the "logs" table on Postgres.
This job is scheduled to run automatically everyday at 18.35 in Windows Task Scheduler.


--PREJOB
--> START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



API_REQUEST tHttpRequest: retrieve sensors data about 2022 from the ARPA api. 
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. All fields are exctracted and imported as String type 
-->
CHANGE_ORA tMap: changes Data field where value is "2022-03-27T02:00:00.000" to "2022-03-27T01:59:00.000" to avoid dupicates related to changing hour
-->
CONVERT_DATATYPES tConvertType: convert each field to the respective datatype (Long,Date,Float,String,Integer). 
					  This final flow will have the role of LEFT table in the later join 
-->
SENS_DATA_2022 tDBInput: retrieves data from table "sens_data_2022" on Postgres. 
				 This flow will have the role of RIGHT table in the later join 
-->
JOIN tMap: performs an INNER JOIN with data from api as left and sens_data_2022 as right; join keys are both idsensore and dataora.
	     the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
	     Generates two output tables:
		- INSERT: obtained from the REJECT of the join operation, i.e., records that are in the api that are not in the db table 
				-->
				INSERT_SHOW tLogRow: shows records to be insert
				-->
				INSERT tDBOutput:	performs an insert operation in Postgres table "sens_data_2022"		
				
		- NOT INSERT: obtained from the MAIN of the join operation, i.e., records present both in the api and the db table.
				  These could need to be updated or not, according to whether their fields are equal. To screen that,
				  a new boolean field "update" is created, checking whether to records are equal (TRUE) or not (FALSE)
					-->
					UPDATE_FILTER tMap: generates an output table with filtered records: only those with "update" field = True pass.
					-->
					UPDATE_SHOW_API tLogRow: shows records to be updated
					-->
					UPDATE tDBOutput: performs an update operation in Postgres table "sens_data_2022"	
	
 
----- EXECUTE AFTER PREVIOUS SUBJOB

SENS_DATA_2022 tDBInput: retrieves data from table "sens_data_2022" on Postgres. 
				 This flow will have the role of LEFT table in the later join
-->
API_REQUEST tHttpRequest: retrieve sensors data about 2022 from the ARPA api. 
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. All fields are exctracted and imported as String type 
-->
CHANGE_ORA tMap: changes Data field where value is "2022-03-27T02:00:00.000" to "2022-03-27T01:59:00.000" to avoid dupicates related to changing hour
-->
CONVERT_DATATYPES tConvertType: convert each field to the respective datatype (Long,Date,Float,String,Integer). 
					  This final flow will have the role of RIGHT table in the later join 
-->
JOIN tMap: performs an INNER JOIN with data from sens_data_2022 as left and data from api as right; join keys are both idsensore and dataora.
	     the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
	     Generates one output table:
		- DELETED: obtained from the REJECT of the join operation, i.e., records that are in the db table that are not in the api
				-->
				DELETE_SHOW tLogRow: shows records to be deleted
				-->
				DELETE tDBOutput:	performs a delete operation in Postgres table "sens_data_2022"		



--POSTJOB
--> PLACEHOLDER tJava: empty tJava functioning as placeholder to connect the followed components (that cannot be directly linked to the PREJOB)
	
	-->
	UPDATE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_UPDATE tMap: creates a boolean variable "update_check" checking whether the number of lines passed in the UPDATE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "update_check" field = True pass.
				     This way, only if some update actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	UPDATE_LOG tDBOutput: performs an insert operation in Postgres table "logs"

	

	-->
	INSERT tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_INSERT tMap: creates a boolean variable "insert_check" checking whether the number of lines passed in the INSERT tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "insert_check" field = True pass.
				     This way, only if some insert actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	INSERT_LOG tDBOutput: performs an insert operation in Postgres table "logs"



	-->
	DELETE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_DELETE tMap: creates a boolean variable "delete_check" checking whether the number of lines passed in the DELETE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "delete_check" field = True pass.
				     This way, only if some delete actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	DELETE_LOG tDBOutput: performs an insert operation in Postgres table "logs"


-->
COMMIT tDBCommit: commits the changes made to Postgres and closes connection to Postgres woneli instance

___________________________________________________________________________________



|| sensorsPositionHistory || OLD DA RIFARE

Performs delta ingestion of stations data retrieved from the ARPA api, i.e. checks for changes in the api (new, updated, and deleted records) 
to be mirrored in the Postgres table "stations". 
It also manages records with inconsistencies by populating the support table "stations_check" by retrieving the last active date and setting the 
datastop for incositent records (i.e. records with storico = "S" but no datastop). Data in the support table are then used to update records in the main table
stations and remove all incosistencies.
The job also saves information to be written in the "logs" table on Postgres.
This job is scheduled to run automatically the first day of each month in Windows Task Scheduler.


--PREJOB
--> START CONNECTION tDBConnection: starts the connection to Postgres instance woneli



--DELETE
STATIONS tDBInput: retrieves data from table "stations" on Postgres.  
-->
MANAGE_NULL tMap: changes Quota field ( imported as String type) where values is "NULL" to "" to avoid NullPointer Errors later
			This flow will have the role of LEFT table in the later join.
-->

API_REQUEST tHttpRequest: retrieve stations data from the ARPA api.
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. 
					     Fields are exctracted and imported with the right type, except for fields Quota which is imported as String type 
					     This to enable to apply the changes made in the following MANAGE_NULL tMap.
-->
MANAGE_NULL tMap: changes Quota field (imported as String type) where values is "NULL" to "" to avoid NullPointer Errors later;
			changes Datastart field where value is "NULL" to "9999-12-31 00:00:00.000" to avoid NullPointer Errors later;
			changes Datastop field where value is "NULL" to "9999-12-31 00:00:00.000" to avoid NullPointer Errors later;
			create field Coordinates by concatenating the two fields Lat and Lng as follows: (lat,lng)
			The output table will have the role of RIGHT table in the later join 
-->
JOIN_COMPARE tMap: performs an INNER JOIN with data from table "stations" as left and data from api as right; join keys are both idsensore and datastart.
	     		 the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
			 Generates one output table:
				- DELETED: obtained from the REJECT of the join operation, i.e., records that are in the db table that are not in the api
					-->
					DELETE_SHOW tLogRow: shows records to be deleted
					-->
					CONVERT_QUOTA tConvertType: converts field Quota from String to Integer while setting empty values to NULL.
					-->
					DELETE tDBOutput:	performs an update operation in Postgres table "stations", by setting field storico to "S" and field datastop to the current date, to close the deleted record.	



----- EXECUTE AFTER PREVIOUS SUBJOB
--INSERT/UPDATE

API_REQUEST tHttpRequest: retrieve stations data from the ARPA api.
-->
EXCTRACT_JSON tExctractJSONFileds: exctracts fields from the json array retrieved from the api. 
					     Fields are exctracted and imported with the right type, except for fields Quota which is imported as String type 
					     This to enable to apply the changes made in the following MANAGE_NULL tMap.
-->
MANAGE_NULL tMap: changes Quota field (imported as String type) where values is "NULL" to "" to avoid NullPointer Errors later;
			changes Datastart field where value is "NULL" to "9999-12-31 00:00:00.000" to avoid NullPointer Errors later;
			changes Datastop field where value is "NULL" to "9999-12-31 00:00:00.000" to avoid NullPointer Errors later;
			create field Coordinates by concatenating the two fields Lat and Lng as follows: (lat,lng)
			The output table will have the role of RIGHT table in the later join.


-->
STATIONS tDBInput: retrieves data from table "stations" on Postgres.  
-->
MANAGE_NULL tMap: changes Quota field ( imported as String type) where values is "NULL" to "" to avoid NullPointer Errors later
			This flow will have the role of LEFT table in the later join

-->
JOIN_COMPARE tMap: performs an INNER JOIN with data from api as left and data table "stations" as right; join keys are both idsensore and datastart.
	     		 the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
			 Generates three tables:
				- NOT INSERT: obtained from the MAIN of the join operation, i.e., records present both in the api and the db table.
				 		  These could need to be updated or not, according to whether their fields are equal. To screen that,
				 		  a new boolean field "update" is created, checking whether to records are equal (TRUE) or not (FALSE)
							-->
							CONVERT_QUOTA tConvertType: converts field Quota from String to Integer while setting empty values to NULL.
							-->
							UPDATE||CLOSE tMap: generates an output table with filtered records: only those with "update" field = True pass.
							-->
							UPDATE tDBOutput: performs an update operation in Postgres table "stations"	
			 	- INSERT: obtained from the REJECT of the join operation, i.e., records that are in the api that are not in the db table 
							-->
							INSERT_SHOW tLogRow: shows records to be insert
							-->
							CONVERT_QUOTA tConvertType: converts field Quota from String to Integer while setting empty values to NULL.
							-->
							INSERT tDBOutput:	performs an insert operation in Postgres table "stations"

				- issues: obtained from the REJECT of the join operation, i.e., records that are in the api that are not in the db table 
					    Exports only datastop and storico to check for inconsistencies (storico = "S" but no datastop).
							-->
							FILTER_ISSUES tFilterRow: filters records where storico = "S" and datastop = "9999-12-31 00:00:00.000", (aka is not present)
							-->
							ISSUES tFlowMeter: stores some job infomation among which the number of rows passed from the FILTER_ISSUES.
	

----- EXECUTE AFTER PREVIOUS SUBJOB
--POPULATE HELP TABLE STATIONS_CHECK

ISSUE_CATCHER tFlowMeterCatcher: retrieves the informations stored by the ISSUE tFlowMeter
-->
ISSUES tLogRow: shows information retrieved by the previous component

- RUN IF -> execute the next component only if the number of rows that passed the filter is > 0 (that is, if there are any issues)

POPULATE_STATIONS_CHECK tDBRow: execute a query populating the Postgres table "stations_check" with records (idsensore, storico) from table stations preseting an issue (storico='S' and datastop='9999-12-31 00:00:00.000')


----- EXECUTE AFTER PREVIOUS SUBJOB
--FILL UP COLUMNS IN HELP TABLE STATIONS_CHECK 

TABLES_NAMES tFileInputDelimeted: retrieves data from a .txt file named tablesNames.txt containing a list of the names of all tables in the db storing sensors data (sens_data_1995, sens_data_2000, ...)
-->
ITER_TABLES tFlowToIterate: iterates over the list of table names present in TABLES_NAMES
-->
PLACEHOLDER tJava: works a placeholder to be able to connect the next component with an IF arrow.
			 This was necessary to be able to reload also the lookuptable in the following job. 
		       In this way, at each iteration, the table stations_check updated at the previous iterations is considered.
   			 This enables to use the IF arrow to exit from the iteration loop when each record in stations_check has been updated with ultima_acquisizione and datastop.
       		 It improves code complexity, as it saves iterations that are not needed.

- RUN IF -> execute the next subjob only if the number of rows retrieved by the STATIONS_CHECK tDBInput component is != 0, that is only if there are still records to update
		Since at the first iteration the "tDBInput_4_NB_LINE" variable containing the "stop" information has not been initialized yet, and this prevents the subjob from starting, 
		the if condition is always satisfied when the "tFlowToIterate_1_CURRENT_ITERATION" = 1.

SENS_DATA_... tDBInput: retrieves data from table "sens_data_..." on Postgres. The name of the table is retrieved from the loop initialzied before. It gets reloaded at each iteration to change table.
-->
STATIONS_CHECK tDBInput: retrieves data from table "stations_check" on Postgres. It gets reloaded at each iteration to get the updated version and to move towards the condition for ending the loop.

-->
JOIN tMap: performs an INNER JOIN with data from table "sens_data_..." as left and data table "stations_check" as right; join key is idsensore.
	     		 the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
			 Generates one table:
				- critical_sensors: obtained from the MAIN of the join operation, i.e., records present both in both tables.
							  If a record is present in both tables it means that it is a record of a critical sensor and that it is the last year in which it was active,
							  bc the iteration goes from 2022 to 1995, so it is iterating backward. 
							-->
							SORT tSortRow: sorts records first according to field idsensore in ascending order and then according to field datatora in descending oreder.
									   Hence the output table will have records "grouped" by idsensore and sorted by date, with the most recent record being the first.
							-->
							KEEP_LAST tUniqueRow: by applying the unique logic, keeps only the first record for each idsensore, which, since the table was first ordered, 
										    corresponds to the last date in which the sensor was active. This wil become the value in column "ultima_acquisizone" in table stations_check.
							-->
							SET_STOPDATE&STORICO tMap: generates an output table with values for each of the stations_check columns. More precisely, it sets:
												- datastop to "9999-12-31 00:00:00.000" if the year in ultima_acquisizione is 2022 (bc this means that the record was wrongly set to "S", bc if it was active during the current year then it cannot be clsoed).
														  31-12 00:00:00.00 of the year in ultima_acquisizione in any other case (that is, it sets the datastop to the last day of the year in which the record was last active)		
												- storico to "N" if the calculated datastop is "9999-12-31 00:00:00.000" to correct and set the record to "active"
							-->							 "S" in any other case
							SENSORS_CEHCK tLogRow: shows which critical sensors (if any) were closed in each year 
							-->
							UPDATE_STATIONS_CHECK tDBOutput: performs an update operation in Postgres table "stations_check"	



----- EXECUTE AFTER PREVIOUS SUBJOB
--MANAGE CRITICAL RECORDS IN TABLE STATIONS

STATIONS tDBInput: retrieves data from table "stations" on Postgres. It works as LEFT table in the later join.
-->
STATIONS_CHECK tDBInput: retrieves data from table "stations_check" on Postgres. It works as RIGHT table in the later join.

-->
JOIN tMap: performs an INNER JOIN with data from table "stations" as left and data table "stations_check" as right; join key is idsensore.
	     		 the criterion used for matching is "UNIQUE MATCH", meaning that records must be equal in all fields to pass the join.
			 Generates one table:
				- solve_issues: obtained from the MAIN of the join operation, i.e., records present both in both tables, which means they have to be updated with the info in stations_check.
							-->
							UPDATE_DATASTOP&STORICO_STATIONS tDBOutput: performs an update operation in Postgres table "stations", setting the correct storico and datastop values for critical sensors.	





--POSTJOB
--> PLACEHOLDER tJava: empty tJava functioning as placeholder to connect the followed components (that cannot be directly linked to the PREJOB)
	
	-->
	UPDATE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_UPDATE tMap: creates a boolean variable "update_check" checking whether the number of lines passed in the UPDATE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "update_check" field = True pass.
				     This way, only if some update actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	UPDATE_LOG tDBOutput: performs an insert operation in Postgres table "logs"

	

	-->
	INSERT tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_INSERT tMap: creates a boolean variable "insert_check" checking whether the number of lines passed in the INSERT tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "insert_check" field = True pass.
				     This way, only if some insert actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	INSERT_LOG tDBOutput: performs an insert operation in Postgres table "logs"



	-->
	DELETE tFixedFlowInput: generates a table with 5 columns: current_date, table_name, operation, number_of_rows, duration_in_seconds
	-->
	CHECK_FOR_DELETE tMap: creates a boolean variable "delete_check" checking whether the number of lines passed in the DELETE tDBOutput is > 0.
				     Generates an output table with filtered records: only those with "delete_check" field = True pass.
				     This way, only if some delete actually happend the log table will be modifed, preventing having log records with number_of_rows = 0.
	-->
	DELETE_LOG tDBOutput: performs an insert operation in Postgres table "logs"


-->
COMMIT tDBCommit: commits the changes made to Postgres and closes connection to Postgres woneli instance

___________________________________________________________________________________
